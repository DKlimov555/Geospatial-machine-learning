{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the images marked as valid per cluster, we pass them through the CNN and extract their feature vectors. the results are stored at a per-country basis. For example, all Malawi feature extractions will go into results/malawi_2016/cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '..'\n",
    "COUNTRIES_DIR = os.path.join(BASE_DIR, 'data', 'countries')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, 'results')\n",
    "CNN_TRAIN_IMAGE_DIR = os.path.join(BASE_DIR, 'data', 'cnn_images')\n",
    "CNN_DIR = os.path.join(BASE_DIR, 'models', 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create results directories for each country if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "for country in ['malawi_2016', 'ethiopia_2015', 'nigeria_2015']:\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, country), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extract with CNN\n",
    "If you have run this step before, you can skip it and run the commented out code in the next section to quick-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(os.path.join(PROCESSED_DIR, 'image_download_actual.csv')) #those are the 33k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>cons_pc</th>\n",
       "      <th>nightlights</th>\n",
       "      <th>country</th>\n",
       "      <th>nightlights_bin</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.09515_35.17229723579403_-17.09515_35.21721...</td>\n",
       "      <td>-17.095150</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.08017807859801_35.17229723579403_-17.09515...</td>\n",
       "      <td>-17.080178</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.125093842803985_35.18726915719602_-17.0951...</td>\n",
       "      <td>-17.125094</td>\n",
       "      <td>35.187269</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.140065764205975_35.20224107859801_-17.0951...</td>\n",
       "      <td>-17.140066</td>\n",
       "      <td>35.202241</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.065206157196016_35.20224107859801_-17.0951...</td>\n",
       "      <td>-17.065206</td>\n",
       "      <td>35.202241</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>1.423239</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.09515_35.17229723579403_-17.09515_35.21721... -17.095150  35.172297   \n",
       "1  -17.08017807859801_35.17229723579403_-17.09515... -17.080178  35.172297   \n",
       "2  -17.125093842803985_35.18726915719602_-17.0951... -17.125094  35.187269   \n",
       "3  -17.140065764205975_35.20224107859801_-17.0951... -17.140066  35.202241   \n",
       "4  -17.065206157196016_35.20224107859801_-17.0951... -17.065206  35.202241   \n",
       "\n",
       "   cluster_lat  cluster_lon   cons_pc  nightlights country  nightlights_bin  \\\n",
       "0    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "1    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "2    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "3    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "4    -17.09515    35.217213  1.423239     0.025206      mw                0   \n",
       "\n",
       "   is_train  \n",
       "0      True  \n",
       "1      True  \n",
       "2     False  \n",
       "3      True  \n",
       "4      True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.head() #variable with training, validation metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda as backend\n"
     ]
    }
   ],
   "source": [
    "#Set up the device for PyTorch (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} as backend')\n",
    "model = torch.load(CNN_DIR, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the classifier part of the CNN model\n",
    "#CNN model typically has two main parts:\n",
    "\n",
    "#Feature Extractor: This is usually a series of convolutional layers that process the input images \n",
    "#and extract features. These layers apply filters to the input to create feature maps that \n",
    "#represent various aspects of the input data.\n",
    "\n",
    "#Classifier: After the feature extractor, the model includes a classifier, which is typically \n",
    "#made up of fully connected layers (also known as linear layers in PyTorch). The classifier's job \n",
    "#is to take the features extracted by the convolutional layers and use them to classify the input \n",
    "#image into various categories (e.g., different types of objects in an image recognition task).\"\"\"\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the first 4 layers for feature extraction\n",
    "model.classifier = model.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea2b094bbe442aa94174c8a4090bcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     49\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 50\u001b[0m         feats[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(inputs),:] \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     51\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m     53\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\u001b[38;5;66;03m#takes a list of transformation commands, applies them sequentially to an image. \u001b[39;00m\n\u001b[0;32m     54\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     55\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     56\u001b[0m         ])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Define image transformations and create a custom dataset class for image loading\n",
    "transformer = transforms.Compose([#takes a list of transformation commands, applies them sequentially to an image. \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# custom dataset for fast image loading and processing\n",
    "# does not follow the usual style of folder -> folder for each class -> image\n",
    "# we just want one folder with images\n",
    "class ForwardPassDataset(torch.utils.data.Dataset):\n",
    "    #forward pass just means running data through the CNN\n",
    "    def __init__(self, image_dir, transformer):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_list = os.listdir(self.image_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_list[index]\n",
    "\n",
    "        # Load image\n",
    "        X = self.filename_to_im_tensor(self.image_dir + '/' + image_name)\n",
    "        \n",
    "        # dataloaders need to return a label, but for the forward pass we don't really care\n",
    "        return X, -1\n",
    "    \n",
    "    def filename_to_im_tensor(self, file):\n",
    "        im = plt.imread(file)[:,:,:3]\n",
    "        im = self.transformer(im)\n",
    "        return im\n",
    "\n",
    "model.eval() # for evaluating, instead of training the model. \n",
    "classes = [0, 1, 2]\n",
    "# shape of final array will be (num_validation_images, 4096)\n",
    "# we also want to record the image each index represents\n",
    "feats = np.zeros(((~df_images['is_train']).sum(), 4096))\n",
    "image_order = []\n",
    "i = 0\n",
    "for c in classes:\n",
    "    # use the validation images to do the forward pass\n",
    "    dataset = ForwardPassDataset(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', str(c)), transformer)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    image_order += dataset.image_list\n",
    "    # forward pass for this class\n",
    "    for inputs, _ in tqdm(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        feats[i:i+len(inputs),:] = outputs.cpu().detach().numpy()\n",
    "        i += len(inputs)\n",
    "        \n",
    "transformer = transforms.Compose([#takes a list of transformation commands, applies them sequentially to an image. \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass_df = pd.DataFrame.from_dict({'image_name': image_order, 'feat_index': np.arange(len(image_order))})\n",
    "forward_pass_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption = pd.merge(left=df_images, right=forward_pass_df, on='image_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have we maintained all validation images?\n",
    "assert len(df_consumption) == (~df_images['is_train']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features\n",
    "For each country, we aggregate the image features per cluster and save them to results/country/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_abbrv = ['mw', 'eth', 'ng']\n",
    "country_dir = ['malawi_2016', 'ethiopia_2015', 'nigeria_2015']\n",
    "\n",
    "for ca, cd in zip(country_abbrv, country_dir):\n",
    "    df_c = df_consumption[df_consumption['country'] == ca]\n",
    "    group = df_c.groupby(['cluster_lat', 'cluster_lon'])\n",
    "    x = np.zeros((len(group), 4096))\n",
    "    cluster_list = [] # the corresponding clusters (lat, lon) to the x aggregate feature array\n",
    "    for i, g in enumerate(group):\n",
    "        lat, lon = g[0]\n",
    "        im_sub = df_consumption[(df_consumption['cluster_lat'] == lat) & (df_consumption['cluster_lon'] == lon)].reset_index(drop=True)\n",
    "        agg_feats = np.zeros((len(im_sub), 4096))\n",
    "        for j, d in im_sub.iterrows():\n",
    "            agg_feats[j,:] = feats[d.feat_index]\n",
    "        agg_feats = agg_feats.mean(axis=0) # averages the features across all images in the cluster\n",
    "\n",
    "        x[i,:] = agg_feats\n",
    "        cluster_list.append([lat, lon])\n",
    "    # save to the correct directory\n",
    "    save_dir = os.path.join(RESULTS_DIR, cd, 'cnn')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(os.path.join(save_dir, 'cluster_feats.npy'), x)\n",
    "    pickle.dump(cluster_list, open(os.path.join(save_dir, 'cluster_order.pkl'), 'wb')) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "nightlight",
   "language": "python",
   "name": "nightlight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
