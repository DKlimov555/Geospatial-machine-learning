{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '..'\n",
    "RANDOM_SEED = 7 # for reproducibility\n",
    "COUNTRIES_DIR = os.path.join(BASE_DIR, 'data', 'countries')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "\n",
    "# these relate to training the CNN to predict nightlights\n",
    "CNN_TRAIN_IMAGE_DIR = os.path.join(BASE_DIR, 'data', 'cnn_images')\n",
    "CNN_SAVE_DIR = os.path.join(BASE_DIR, 'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(CNN_TRAIN_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(CNN_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "After doing this once, you can skip to the training if the script broke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actually downloaded: 31226, expected: 30649\n"
     ]
    }
   ],
   "source": [
    "df_download = pd.read_csv(os.path.join(PROCESSED_DIR, 'image_download_locs.csv'))\n",
    "downloaded = os.listdir(os.path.join(COUNTRIES_DIR, 'malawi_2019', 'images')) + \\\n",
    "            os.listdir(os.path.join(COUNTRIES_DIR, 'ethiopia_2019', 'images')) + \\\n",
    "            os.listdir(os.path.join(COUNTRIES_DIR, 'nigeria_2019', 'images'))\n",
    "\n",
    "print(f\"actually downloaded: {len(downloaded)}, expected: {len(df_download)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>cons_pc</th>\n",
       "      <th>nightlights</th>\n",
       "      <th>country</th>\n",
       "      <th>nightlights_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.0935306549072_35.20822373164363_-17.093530...</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.208224</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.07855873350521_35.20822373164363_-17.09353...</td>\n",
       "      <td>-17.078559</td>\n",
       "      <td>35.208224</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.123474497711186_35.22319565304562_-17.0935...</td>\n",
       "      <td>-17.123474</td>\n",
       "      <td>35.223196</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.138446419113176_35.23816757444761_-17.0935...</td>\n",
       "      <td>-17.138446</td>\n",
       "      <td>35.238168</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.063586812103217_35.23816757444761_-17.0935...</td>\n",
       "      <td>-17.063587</td>\n",
       "      <td>35.238168</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.0935306549072_35.20822373164363_-17.093530... -17.093531  35.208224   \n",
       "1  -17.07855873350521_35.20822373164363_-17.09353... -17.078559  35.208224   \n",
       "2  -17.123474497711186_35.22319565304562_-17.0935... -17.123474  35.223196   \n",
       "3  -17.138446419113176_35.23816757444761_-17.0935... -17.138446  35.238168   \n",
       "4  -17.063586812103217_35.23816757444761_-17.0935... -17.063587  35.238168   \n",
       "\n",
       "   cluster_lat  cluster_lon   cons_pc  nightlights country  nightlights_bin  \n",
       "0   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "1   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "2   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "3   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "4   -17.093531    35.253139  1.994561     0.034344      mw                0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_download.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-10.004834563858024_33.95477752928161_-10.049750328064_33.9697494506836.png',\n",
       " '-10.014142224746722_33.441672513442995_-9.99917030334473_33.426700592041.png',\n",
       " '-10.019806485260016_33.924833686477626_-10.049750328064_33.9697494506836.png',\n",
       " '-10.019806485260016_33.93980560787962_-10.049750328064_33.9697494506836.png']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_download['row'] = np.arange(len(df_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 30649\n",
      "After filtering: 30649\n",
      "actually downloaded: 31226, expected: 30649\n"
     ]
    }
   ],
   "source": [
    "#Try to remove additional rows\n",
    "\n",
    "print(\"Before filtering:\", df_download.shape[0])\n",
    "df_download = df_download[df_download['image_name'].isin(downloaded)]\n",
    "print(\"After filtering:\", df_download.shape[0])\n",
    "\n",
    "print(f\"actually downloaded: {len(downloaded)}, expected: {len(df_download)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicate files.\n",
      "Duplicate files: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, the rest of the duplicate-checking code\n",
    "# Check if there are duplicates by comparing lengths\n",
    "unique_files = set(downloaded)\n",
    "if len(unique_files) != len(downloaded):\n",
    "    print(f\"There are {len(downloaded) - len(unique_files)} duplicate files.\")\n",
    "if len(unique_files) == len(downloaded):\n",
    "    print(f\"There are no duplicate files.\")    \n",
    "    \n",
    "# Identify the duplicate files\n",
    "from collections import Counter\n",
    "file_counts = Counter(downloaded)\n",
    "duplicates = [file for file, count in file_counts.items() if count > 1]\n",
    "\n",
    "print(f\"Duplicate files: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with names not in 'downloaded': 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify that there are no rows in df_download where image_name is not in the downloaded list. (should be 0)\n",
    "non_existent = df_download[~df_download['image_name'].isin(downloaded)]\n",
    "print(\"Rows with names not in 'downloaded':\", non_existent.shape[0])\n",
    "\n",
    "#compare lengths\n",
    "len(downloaded) - len(df_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in 'downloaded' not in 'df_download['image_name']': 577\n"
     ]
    }
   ],
   "source": [
    "#number of rows that are in downloaded, but not in image_name\n",
    "\n",
    "image_name_set = set(df_download['image_name'])\n",
    "downloaded_set = set(downloaded)\n",
    "\n",
    "# Find elements in 'downloaded' that are not in 'image_name'\n",
    "not_in_image_name = downloaded_set - image_name_set\n",
    "\n",
    "# Count the number of such elements\n",
    "count_not_in_image_name = len(not_in_image_name)\n",
    "\n",
    "print(f\"Number of items in 'downloaded' not in 'df_download['image_name']': {count_not_in_image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in 'downloaded' missing in 'df_download': 577\n"
     ]
    }
   ],
   "source": [
    "#drop these rows above\n",
    "# Keep only rows in df_download where 'image_name' is in 'downloaded'\n",
    "df_download = df_download[df_download['image_name'].isin(downloaded)]\n",
    "\n",
    "#check if worked\n",
    "missing_in_df = [name for name in downloaded if name not in df_download['image_name'].values]\n",
    "print(\"Items in 'downloaded' missing in 'df_download':\", len(missing_in_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#idx_not_download = df_download.set_index('image_name').drop(downloaded)['row'].values.tolist()\n",
    "\n",
    "#df_download.drop(idx_not_download, inplace=True)\n",
    "\n",
    "#original script had fewer items in download than in image_name.\n",
    "#now produces error bc there are more items in downloaded than in df_download. \n",
    "#should not be relevant at this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_download.drop('row', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41511957975790403, 0.37117034813533883, 0.21371007210675716)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the distribution\n",
    "(df_download['nightlights_bin']==0).mean(), (df_download['nightlights_bin']==1).mean(), (df_download['nightlights_bin']==2).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split images into train/valid.\n",
    "Each cluster will contribute 80% of images for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_download.reset_index(drop=True, inplace=True) #Make sure DataFrame has a clean, sequential index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>cons_pc</th>\n",
       "      <th>nightlights</th>\n",
       "      <th>country</th>\n",
       "      <th>nightlights_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.0935306549072_35.20822373164363_-17.093530...</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.208224</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.07855873350521_35.20822373164363_-17.09353...</td>\n",
       "      <td>-17.078559</td>\n",
       "      <td>35.208224</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.123474497711186_35.22319565304562_-17.0935...</td>\n",
       "      <td>-17.123474</td>\n",
       "      <td>35.223196</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.138446419113176_35.23816757444761_-17.0935...</td>\n",
       "      <td>-17.138446</td>\n",
       "      <td>35.238168</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.063586812103217_35.23816757444761_-17.0935...</td>\n",
       "      <td>-17.063587</td>\n",
       "      <td>35.238168</td>\n",
       "      <td>-17.093531</td>\n",
       "      <td>35.253139</td>\n",
       "      <td>1.994561</td>\n",
       "      <td>0.034344</td>\n",
       "      <td>mw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.0935306549072_35.20822373164363_-17.093530... -17.093531  35.208224   \n",
       "1  -17.07855873350521_35.20822373164363_-17.09353... -17.078559  35.208224   \n",
       "2  -17.123474497711186_35.22319565304562_-17.0935... -17.123474  35.223196   \n",
       "3  -17.138446419113176_35.23816757444761_-17.0935... -17.138446  35.238168   \n",
       "4  -17.063586812103217_35.23816757444761_-17.0935... -17.063587  35.238168   \n",
       "\n",
       "   cluster_lat  cluster_lon   cons_pc  nightlights country  nightlights_bin  \n",
       "0   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "1   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "2   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "3   -17.093531    35.253139  1.994561     0.034344      mw                0  \n",
       "4   -17.093531    35.253139  1.994561     0.034344      mw                0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_download.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_download['is_train'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "groups = df_download.groupby(['cluster_lat', 'cluster_lon'])\n",
    "\n",
    "df_download['is_train'] = True\n",
    "\n",
    "for _, g in groups:\n",
    "    n_ims = len(g)\n",
    "    n_train = int(0.8 * n_ims)\n",
    "    n_valid = n_ims - n_train\n",
    "    valid_choices = np.random.choice(np.arange(n_ims), replace=False, size=n_valid)\n",
    "    current_index = g.index[valid_choices]\n",
    "    \n",
    "    # Set 'is_train' to False for the valid set directly using loc\n",
    "    df_download.loc[current_index, 'is_train'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7643642533198473"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_download['is_train'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save this new dataframe\n",
    "df_download.to_csv(os.path.join(PROCESSED_DIR, 'image_download_actual.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'train2'), exist_ok=False) #training dataset\n",
    "os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid'), exist_ok=False) #validation dataset\n",
    "\n",
    "labels = ['0', '1', '2']\n",
    "for l in labels:\n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'train2', l), exist_ok=False)\n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', l), exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = df_download[df_download['is_train']]\n",
    "v = df_download[~df_download['is_train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23427, 7222)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t), len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying train images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee841789a4b4456b9acccd601fc5125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying valid images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584930960a747cb8da91a3eb15eb12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# uses symlinking to save disk space\n",
    "# i.e. creates shortcuts, instead of new files\n",
    "print('copying train images')\n",
    "for im_name, nl, country in tqdm(zip(t['image_name'], t['nightlights_bin'], t['country']), total=len(t)):\n",
    "    country_dir = None\n",
    "    if country == 'mw':\n",
    "        country_dir = 'malawi_2019'\n",
    "    elif country == 'eth':\n",
    "        country_dir = 'ethiopia_2019'\n",
    "    elif country == 'ng':\n",
    "        country_dir = 'nigeria_2019'\n",
    "    else:\n",
    "        print(f\"no match for country {country}\")\n",
    "        raise ValueError()\n",
    "    src = os.path.abspath(os.path.join(COUNTRIES_DIR, country_dir, 'images', im_name))\n",
    "    dest = os.path.join(CNN_TRAIN_IMAGE_DIR, 'train2', str(nl), im_name)\n",
    "    if os.symlink(src, dest, target_is_directory = False) != None:\n",
    "        print(\"error creating symlink\")\n",
    "        raise ValueError()\n",
    "\n",
    "print('copying valid images')\n",
    "for im_name, nl, country in tqdm(zip(v['image_name'], v['nightlights_bin'], v['country']), total=len(v)):\n",
    "    country_dir = None\n",
    "    if country == 'mw':\n",
    "        country_dir = 'malawi_2019'\n",
    "    elif country == 'eth':\n",
    "        country_dir = 'ethiopia_2019'\n",
    "    elif country == 'ng':\n",
    "        country_dir = 'nigeria_2019'\n",
    "    else:\n",
    "        print(f\"no match for country {country}\")\n",
    "        raise ValueError()\n",
    "    src = os.path.abspath(os.path.join(COUNTRIES_DIR, country_dir, 'images', im_name))\n",
    "    dest = os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', str(nl), im_name)\n",
    "    if os.symlink(src, dest, target_is_directory = False) != None:\n",
    "        print(\"error creating symlink\")\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9445, 8742, 5240]\n",
      "[0.4031672856106202, 0.3731591753105391, 0.22367353907884066]\n",
      "23427\n",
      "\n",
      "compare match:\n",
      "nightlights_bin\n",
      "0    9445\n",
      "1    8742\n",
      "2    5240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shows count distribution in train folder, make sure this matches above\n",
    "# i.e. 80% of each cluster should be in train\n",
    "\n",
    "counts = []\n",
    "for l in ['0', '1', '2']:\n",
    "    counts.append(len(os.listdir(os.path.join(CNN_TRAIN_IMAGE_DIR, 'train', l))))\n",
    "print(counts)\n",
    "print([c/sum(counts) for c in counts])\n",
    "print(sum(counts))\n",
    "\n",
    "print(\"\\ncompare match:\")\n",
    "# Check distribution in training set\n",
    "print(t['nightlights_bin'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3278, 2634, 1310]\n",
      "[0.4538908889504292, 0.36471891442813625, 0.18139019662143452]\n",
      "7222\n",
      "\n",
      "compare:\n",
      "nightlights_bin\n",
      "0    3278\n",
      "1    2634\n",
      "2    1310\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shows count distribution in valid folder\n",
    "counts = []\n",
    "for l in ['0', '1', '2']:\n",
    "    counts.append(len(os.listdir(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', l))))\n",
    "print(counts)\n",
    "print([c/sum(counts) for c in counts])\n",
    "print(sum(counts))\n",
    "\n",
    "print(\"\\ncompare:\")\n",
    "# Check distribution in validation set\n",
    "print(v['nightlights_bin'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Heavily adapted from the PyTorch CNN training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top level data directory.\n",
    "\n",
    "data_dir = CNN_TRAIN_IMAGE_DIR\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for, first 10 will be training the new layers, last 10 the whole model\n",
    "num_epochs = 20\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    \"\"\"\n",
    "    Initialize and modify a CNN model.\n",
    "\n",
    "    Args:\n",
    "    model_name (str): Name of the model architecture (e.g., 'vgg').\n",
    "    num_classes (int): Number of classes for the final output layer.\n",
    "    feature_extract (bool): If True, model is used as a fixed feature extractor, \n",
    "                            with gradients not being updated.\n",
    "    use_pretrained (bool): If True, use a pre-trained model; otherwise, initialize from scratch.\n",
    "\n",
    "    Returns:\n",
    "    model_ft: Modified CNN model.\n",
    "    input_size: Expected input size for the model.\n",
    "    \"\"\"\n",
    "    # Initialize the model; if 'use_pretrained' is True, load pre-trained weights\n",
    "    model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "\n",
    "    # Freeze model parameters if feature extraction is intended\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "    # Get the number of input features for the classifier layer\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "\n",
    "    # Replace the last classifier layer with a new one matching the number of classes\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Define the input size expected by this model (224x224 for VGG)\n",
    "    input_size = 224\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting: #If True, parameters are frozen (not updated during training).\n",
    "        # Iterate over all parameters in the model and freeze them\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False #features frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dima\\miniconda3\\envs\\nightlight\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dima\\miniconda3\\envs\\nightlight\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can modify the classifier part of the model by doing this\n",
    "# model_ft.classifier = nn.Sequential(\n",
    "#     nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.Linear(in_features=4096, out_features=256, bias=True),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.Linear(in_features=256, out_features=3, bias=True),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'valid']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "\n",
    "#is CUDA available?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch > 10:\n",
    "            # fine tune whole model\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = optim.SGD(model_ft.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'valid':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dima\\\\predicting-poverty-replication\\\\scripts'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load image: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dima\\\\predicting-poverty-replication\\\\data\\\\cnn_images\\\\train\\\\0\\\\-10.004834563858024_33.93980560787962_-10.049750328064_33.9697494506836.png'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Specify the path to one of your images\n",
    "image_path = r'C:\\Users\\Dima\\predicting-poverty-replication\\data\\cnn_images\\train\\0\\-10.004834563858024_33.93980560787962_-10.049750328064_33.9697494506836.png'\n",
    "\n",
    "# Try to open the image\n",
    "try:\n",
    "    img = Image.open(image_path)\n",
    "    print(\"Image loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at ..\\data\\cnn_images\\train\\0\\-15.740327646774308_35.620852658706696_-15.7552995681763_35.6058807373047.png is readable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '..\\\\data\\\\cnn_images\\\\train\\\\0\\\\-15.740327646774308_35.620852658706696_-15.7552995681763_35.6058807373047.png'\n",
    "if os.access(file_path, os.R_OK):\n",
    "    print(f\"The file at {file_path} is readable.\")\n",
    "else:\n",
    "    print(f\"The file at {file_path} is not readable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(CNN_SAVE_DIR, 'trained_model.pt')\n",
    "assert not os.path.isfile(path), print('A model is already saved at this location')\n",
    "print(f'Saving model to {path}')\n",
    "torch.save(model_ft, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can run below if you want to see the final accuracy on nightlights over the train set\n",
    "model_ft.eval()   # Set model to evaluate mode\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over data.\n",
    "for inputs, labels in tqdm(dataloaders_dict['train']):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_ft(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    total += len(preds)\n",
    "        \n",
    "print(running_corrects.double()/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "nightlight",
   "language": "python",
   "name": "nightlight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
